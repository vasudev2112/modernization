package com.example.etl;
 
import org.apache.spark.sql.*;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.expressions.WindowSpec;
import org.apache.spark.sql.types.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
 
import java.util.Arrays;
 
import static org.apache.spark.sql.functions.*;
 
 
public class UserMetricsJob {
 
    private static final Logger log = LoggerFactory.getLogger(UserMetricsJob.class);
 
    public static void main(String[] args) {
        String eventsPath = getArg(args, "--events", "sample_data/events.csv");
        String usersPath  = getArg(args, "--users",  "sample_data/users.csv");
        String outPath    = getArg(args, "--out",    "out/user_metrics_parquet");
        String minDate    = getArg(args, "--from",   "1970-01-01");
        String maxDate    = getArg(args, "--to",     "2100-01-01");
        boolean useUdf    = Boolean.parseBoolean(getArg(args, "--useUdf", "false"));
 
        SparkSession spark = SparkSession.builder()
                .appName("UserMetricsJob")
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.shuffle.partitions", "8")
                .getOrCreate();
 
        try {
            log.info("Starting job with events={}, users={}, out={}, window=[{}, {}], useUdf={}",
                    eventsPath, usersPath, outPath, minDate, maxDate, useUdf);
 
            Dataset<Row> events = loadEvents(spark, eventsPath);
            Dataset<Row> users  = loadUsers(spark, usersPath);
 
            Dataset<Row> transformed = transform(events, users, minDate, maxDate, useUdf);
 
           
            transformed
                    .coalesce(1) 
                    .write()
                    .mode(SaveMode.Overwrite)
                    .format("parquet")
                    .save(outPath);
 
            
            transformed.show(false);
 
            log.info("Job completed successfully. Output: {}", outPath);
        } catch (AnalysisException ae) {
            log.error("Spark analysis error: {}", ae.getMessage(), ae);
            throw new RuntimeException("Analysis exception during job run", ae);
        } catch (Exception e) {
            log.error("Unexpected error: {}", e.getMessage(), e);
            throw new RuntimeException("Unhandled exception", e);
        } finally {
            spark.stop();
        }
    }
 
    
 
    private static Dataset<Row> loadEvents(SparkSession spark, String path) {
        StructType schema = new StructType(new StructField[]{
                new StructField("user_id", DataTypes.StringType, true, Metadata.empty()),
                new StructField("event_type", DataTypes.StringType, true, Metadata.empty()),
                new StructField("score", DataTypes.IntegerType, true, Metadata.empty()),
                new StructField("amount", DataTypes.DoubleType, true, Metadata.empty()),
                new StructField("ts", DataTypes.TimestampType, true, Metadata.empty())
        });
 
        return spark.read()
                .option("header", "true")
                .schema(schema)
                .csv(path);
    }
 
    private static Dataset<Row> loadUsers(SparkSession spark, String path) {
        StructType schema = new StructType(new StructField[]{
                new StructField("user_id", DataTypes.StringType, false, Metadata.empty()),
                new StructField("country", DataTypes.StringType, true, Metadata.empty())
        });
 
        return spark.read()
                .option("header", "true")
                .schema(schema)
                .csv(path);
    }
 
   
    public static Dataset<Row> transform(
            Dataset<Row> events,
            Dataset<Row> users,
            String minDateInclusive,
            String maxDateExclusive,
            boolean useUdfBucket
    ) {
        Column inWindow = col("ts").geq(to_timestamp(lit(minDateInclusive)))
                .and(col("ts").lt(to_timestamp(lit(maxDateExclusive))));
 
        Dataset<Row> filtered = events
                .filter(col("event_type").isin("click", "purchase"))
                .filter(inWindow);
 
       
        if (useUdfBucket) {
            sparkRegisterBucketUdf(filtered.sparkSession());
            filtered = filtered.withColumn("score_bucket", callUDF("bucketScore", col("score")));
        } else {
            filtered = filtered.withColumn(
                    "score_bucket",
                    when(col("score").isNull(), lit("unknown"))
                            .when(col("score").geq(lit(80)), lit("high"))